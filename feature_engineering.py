import numpy as np
import re
from datetime import datetime, timedelta
import pandas as pd
import os

# Try to load precomputed daily data `cleaned.csv` if present. This file is
# generated by `daily_features.py` and contains historical daily trend stats
# and a `will_trend_tomorrow` label. Caching it here keeps prediction fast.
_CLEANED_DF = None
_CLEANED_PATH = os.path.join(os.path.dirname(__file__), "cleaned.csv")
if os.path.exists(_CLEANED_PATH):
    try:
        _CLEANED_DF = pd.read_csv(_CLEANED_PATH, parse_dates=["trend_date"])
        # normalize trend_name for matching
        _CLEANED_DF["trend_key"] = _CLEANED_DF["trend_name"].astype(str).str.lower().str.strip()
    except Exception:
        _CLEANED_DF = None

    # Precompute global popularity stats for quick lookups
    _GLOBAL_STATS = {}
    if _CLEANED_DF is not None:
        try:
            grp = _CLEANED_DF.groupby("trend_key").agg({"total_volume": "sum", "trend_date": "nunique"})
            grp = grp.rename(columns={"trend_date": "appearances"}).reset_index()
            grp["volume_per_appearance"] = grp["total_volume"] / (grp["appearances"] + 1e-9)
            grp["popularity_rank"] = grp["total_volume"].rank(pct=True)
            # map for quick lookup
            _GLOBAL_STATS = grp.set_index("trend_key").to_dict(orient="index")
        except Exception:
            _GLOBAL_STATS = {}

# Build embedding centroids per historical trend_name (if bert_embeddings.csv present).
# This enables nearest-neighbor feature transfer for unseen hashtags so predictions
# vary based on semantic similarity instead of falling back to identical defaults.
_EMBED_CENTROIDS = {}
_EMBED_KEYS = []
_EMBED_MATRIX = None
_EMBED_STATS = {}
_EMBED_PATH = os.path.join(os.path.dirname(__file__), "bert_embeddings.csv")
if os.path.exists(_EMBED_PATH):
    try:
        # read header to find embedding columns
        tmp = pd.read_csv(_EMBED_PATH, nrows=1)
        emb_cols = [c for c in tmp.columns if c.startswith("emb")]
        usecols = ["trend_name"] + emb_cols
        df_emb = pd.read_csv(_EMBED_PATH, usecols=usecols)
        df_emb["trend_key"] = df_emb["trend_name"].astype(str).str.lower().str.strip()
        # compute centroid per trend_key
        grp = df_emb.groupby("trend_key")[emb_cols].mean()
        _EMBED_KEYS = list(grp.index)
        _EMBED_MATRIX = grp.values.astype(float)
        # store centroids in dict for lookup
        for i, k in enumerate(_EMBED_KEYS):
            _EMBED_CENTROIDS[k] = _EMBED_MATRIX[i]
        # also compute simple stats per trend_key from cleaned.csv if available
        if _CLEANED_DF is not None:
            stats = _CLEANED_DF.groupby("trend_key").agg({"total_volume": "sum", "trend_date": "nunique", "avg_rank": "mean"}).rename(columns={"trend_date": "appearances"}).to_dict(orient="index")
            for k, v in stats.items():
                _EMBED_STATS[k] = v
    except Exception:
        _EMBED_CENTROIDS = {}
        _EMBED_MATRIX = None

# -------------------------
# Basic NLP helpers
# -------------------------
def detect_category(tag):
    tag_l = tag.lower()
    return {
        "is_sports": int(any(w in tag_l for w in ["vs", "match", "fc", "cricket", "cup", "league"])),
        "is_political": int(any(w in tag_l for w in ["modi", "bjp", "congress", "election", "vote"])),
        "is_event": int(any(w in tag_l for w in ["festival", "day", "birthday", "anniversary", "award", "launch"]))
    }

def sentiment_from_text(tag):
    pos = ["love", "great", "happy", "win", "amazing"]
    neg = ["hate", "bad", "anger", "worst", "sad"]
    score = 0
    tl = tag.lower()
    for p in pos:
        if p in tl:
            score += 1
    for n in neg:
        if n in tl:
            score -= 1
    return score

# -------------------------
# Main feature builder
# -------------------------
def add_features(hashtag, emb):
    """Build features for a single hashtag + embedding using historical `cleaned.csv`.

    If `cleaned.csv` is not available, falls back to simple heuristics with
    conservative defaults.
    """
    now = datetime.utcnow()
    cat = detect_category(hashtag)
    sentiment = sentiment_from_text(hashtag)

    # prepare defaults
    defaults = {
        "avg_rank": 10.0,
        "min_rank": 10.0,
        "max_rank": 10.0,
        "total_volume": 0.0,
        "appearances": 0,
        "is_most_tweeted": 0,
        "is_longest_trending": 0,
        "days_diff": 0,
        "prev_rank": 10.0,
        "prev_volume": 0.0,
        "rank_change": 0.0,
        "volume_change": 0.0,
        "day_of_week": now.weekday(),
        "is_weekend": int(now.weekday() in [5, 6]),
        "month": now.month,
        "days_since_last_trend": 999,
        "recent_streak": 0,
        "momentum_score": 0.0,
        "volatility": 0.0,
        "trend_length": 0,
        "has_numbers": int(bool(re.search(r"\d", hashtag))),
        "sentiment_score": sentiment,
        "rolling_avg_volume_3": 0.0,
        "rolling_avg_rank_3": 10.0,
        "trend_freq_7d": 0,
        "is_sports": cat["is_sports"],
        "is_political": cat["is_political"],
        "is_event": cat["is_event"],
    }

    features = dict(defaults)

    # Generic / greeting detection to avoid predicting trivial greetings as trending
    generic_keywords = {"goodmorning", "goodnight", "good_night", "gm", "gn", "goodafternoon", "goodbye", "buenosdias", "hbd", "happybirthday"}
    tag_norm = hashtag.lower().strip()
    features["is_generic"] = int(any(k in tag_norm for k in generic_keywords))

    # If historical cleaned data is available, compute features from it
    if _CLEANED_DF is not None:
        df = _CLEANED_DF.copy()

        # normalize query key and try matching with/without leading '#'
        q_keys = [hashtag.lower().strip(), f"#{hashtag.lower().strip()}"]
        df_tag = df[df["trend_key"].isin(q_keys)].sort_values("trend_date")

        if not df_tag.empty:
            # last observed date for this tag
            last_date = df_tag["trend_date"].max()
            last_row = df_tag[df_tag["trend_date"] == last_date].iloc[-1]

            # current-day style features from last observed day
            features["avg_rank"] = float(last_row.get("avg_rank", defaults["avg_rank"]))
            features["min_rank"] = float(last_row.get("min_rank", defaults["min_rank"]))
            features["max_rank"] = float(last_row.get("max_rank", defaults["max_rank"]))
            features["total_volume"] = float(last_row.get("total_volume", defaults["total_volume"]))

            # appearances = number of distinct days the tag appeared
            features["appearances"] = int(df_tag["trend_date"].nunique())

            # trend_length = total days seen historically
            features["trend_length"] = int(features["appearances"])

            # prev day features
            prev_date = last_date - timedelta(days=1)
            prev_rows = df_tag[df_tag["trend_date"] == prev_date]
            if not prev_rows.empty:
                prev = prev_rows.iloc[-1]
                features["prev_rank"] = float(prev.get("avg_rank", defaults["prev_rank"]))
                features["prev_volume"] = float(prev.get("total_volume", defaults["prev_volume"]))
                features["rank_change"] = features["prev_rank"] - features["avg_rank"]
                features["volume_change"] = features["prev_volume"] - features["total_volume"]
            else:
                # fallbacks
                features["prev_rank"] = features["avg_rank"]
                features["prev_volume"] = features["total_volume"]
                features["rank_change"] = 0.0
                features["volume_change"] = 0.0

            # days_since_last_trend: days between last_date and previous appearance
            prior = df_tag[df_tag["trend_date"] < last_date]
            if not prior.empty:
                prior_date = prior["trend_date"].max()
                features["days_since_last_trend"] = int((last_date - prior_date).days)
            else:
                features["days_since_last_trend"] = 999

            # recent streak: consecutive days up to last_date
            streak = 0
            check_date = last_date
            while True:
                if check_date in set(df_tag["trend_date"]):
                    streak += 1
                    check_date = check_date - timedelta(days=1)
                else:
                    break
            features["recent_streak"] = int(streak)

            # trend_freq_7d: count appearances in last 7 days ending at last_date
            week_start = last_date - timedelta(days=6)
            features["trend_freq_7d"] = int(df_tag[(df_tag["trend_date"] >= week_start) & (df_tag["trend_date"] <= last_date)]["trend_date"].nunique())

            # rolling averages / volatility using last 3 days
            window_start = last_date - timedelta(days=2)
            window = df_tag[(df_tag["trend_date"] >= window_start) & (df_tag["trend_date"] <= last_date)]
            if not window.empty:
                features["rolling_avg_volume_3"] = float(window["total_volume"].mean())
                features["rolling_avg_rank_3"] = float(window["avg_rank"].mean())
                features["volatility"] = float(window["avg_rank"].std(ddof=0) if window["avg_rank"].nunique() > 1 else 0.0)
            else:
                features["rolling_avg_volume_3"] = features["total_volume"]
                features["rolling_avg_rank_3"] = features["avg_rank"]
                features["volatility"] = 0.0

            # momentum: relative change in rolling volume vs previous window
            prev_window_start = window_start - timedelta(days=3)
            prev_window = df_tag[(df_tag["trend_date"] >= prev_window_start) & (df_tag["trend_date"] < window_start)]
            if not prev_window.empty and features["rolling_avg_volume_3"] > 0:
                features["momentum_score"] = float((features["rolling_avg_volume_3"] - prev_window["total_volume"].mean()) / (prev_window["total_volume"].mean() + 1e-9))
            else:
                features["momentum_score"] = 0.0

            # is_most_tweeted: whether today's volume is top among that day's tags
            day_df = df[df["trend_date"] == last_date]
            if not day_df.empty:
                max_vol = day_df["total_volume"].max()
                features["is_most_tweeted"] = int(features["total_volume"] >= max_vol)

            # is_longest_trending: compare this trend's length to median trend length
            trend_lengths = df.groupby("trend_key")["trend_date"].nunique()
            median_len = int(trend_lengths.median()) if not trend_lengths.empty else 0
            features["is_longest_trending"] = int(features["trend_length"] >= median_len)

            # populate global stats if present
            g = _GLOBAL_STATS.get(last_row["trend_key"], None)
            if g:
                features["global_total_volume"] = float(g.get("total_volume", 0.0))
                features["global_appearances"] = int(g.get("appearances", 0))
                features["global_popularity_pct"] = float(g.get("popularity_rank", 0.0))
                features["volume_per_appearance"] = float(g.get("volume_per_appearance", 0.0))
            else:
                features["global_total_volume"] = features["total_volume"]
                features["global_appearances"] = features["appearances"]
                features["global_popularity_pct"] = 0.0
                features["volume_per_appearance"] = features["total_volume"]
        else:
            # If tag not seen historically, attempt to synthesize features from
            # nearest historical hashtags using embedding similarity (if available).
            try:
                if _EMBED_MATRIX is not None and len(emb) > 0:
                    import numpy.linalg as la
                    v = np.asarray(emb, dtype=float)
                    # normalize
                    v_norm = v / (la.norm(v) + 1e-9)
                    mat = _EMBED_MATRIX.astype(float)
                    mat_norm = mat / (np.linalg.norm(mat, axis=1, keepdims=True) + 1e-9)
                    sims = mat_norm @ v_norm
                    # find top-k
                    k = 3
                    top_idx = sims.argsort()[::-1][:k]
                    top_keys = [ _EMBED_KEYS[i] for i in top_idx ]
                    top_sims = sims[top_idx]
                    if top_sims.max() > 0.35:
                        # weighted average of neighbor stats
                        w = top_sims / (top_sims.sum() + 1e-9)
                        agg_total_vol = 0.0
                        agg_avg_rank = 0.0
                        agg_appear = 0
                        for i, key in enumerate(top_keys):
                            s = _EMBED_STATS.get(key)
                            if s:
                                agg_total_vol += w[i] * float(s.get("total_volume", 0.0))
                                agg_avg_rank += w[i] * float(s.get("avg_rank", 10.0))
                                agg_appear += w[i] * float(s.get("appearances", 0))
                        features["total_volume"] = float(agg_total_vol)
                        features["avg_rank"] = float(agg_avg_rank)
                        features["appearances"] = int(round(agg_appear))
                        features["global_total_volume"] = float(agg_total_vol)
                        features["global_appearances"] = int(round(agg_appear))
                        features["volume_per_appearance"] = float(agg_total_vol / (agg_appear + 1e-9))
                        features["global_popularity_pct"] = float((np.mean(top_sims)))
            except Exception:
                pass

    # Add embedding dims. If embedding shorter than expected, pad with zeros.
    for i in range(len(emb)):
        features[f"emb{i}"] = float(emb[i])

    # additional engineered features for model stability
    # log-transform skewed volumes
    features["log_total_volume"] = float(np.log1p(features.get("total_volume", 0.0)))
    features["log_global_total_volume"] = float(np.log1p(features.get("global_total_volume", 0.0)))
    # volume per appearance (smoothed)
    features["volume_per_appearance"] = float(features.get("volume_per_appearance", features.get("total_volume", 0.0)))
    # if generic greeting, down-weight momentum and set a flag
    if features.get("is_generic", 0) == 1:
        features["momentum_score"] = 0.0

    return features
